# Re:Search â€” Python Backend

AI researcher agent with a Universal Node/Edge knowledge graph, RAG ingestion pipeline, and autonomous LangGraph agent.

## Quick Start

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Verify scaffold
python -c "import backend; print('scaffold ok')"
python cli/main.py --help
```

## Project Structure

```
Search/
â”œâ”€â”€ docs/                    # Design documentation
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ config.py            # Centralised settings (env-driven)
â”‚   â”œâ”€â”€ db/                  # Phase 1: SQLite + FTS5 + sqlite-vec
â”‚   â”œâ”€â”€ scraper/             # Phase 2: httpx + trafilatura + playwright
â”‚   â”œâ”€â”€ rag/                 # Phase 3: chunker + embedder + ingestor
â”‚   â”œâ”€â”€ agent/               # Phase 4: LangGraph researcher graph
â”‚   â””â”€â”€ api/                 # Phase 5: FastAPI HTTP layer
â”œâ”€â”€ cli/
â”‚   â””â”€â”€ main.py              # Typer CLI entry-point
â”œâ”€â”€ tests/                   # pytest test suite
â”œâ”€â”€ requirements.txt
â””â”€â”€ pyproject.toml
```

## Configuration

Copy `.env.example` (if provided) to `.env` and set:

| Variable | Default | Description |
|---|---|---|
| `RESEARCH_WORKSPACE` | `~/.research_data` | Directory for the SQLite DB |
| `EMBEDDING_PROVIDER` | `ollama` | `ollama` or `openai` |
| `OLLAMA_BASE_URL` | `http://localhost:11434` | Ollama server URL |
| `OLLAMA_EMBED_MODEL` | `nomic-embed-text` | Embedding model |
| `OLLAMA_CHAT_MODEL` | `llama3.2` | Chat/reasoning model |
| `OPENAI_API_KEY` | _(unset)_ | Required if using OpenAI |
| `LLM_PROVIDER` | `ollama` | `ollama` or `openai` |

## Running Tests

```bash
pytest tests/ -v --tb=short
```

## Build Phases

| Phase | Status | Description |
|---|---|---|
| 0 | âœ… Complete | Project scaffold |
| 1 | ðŸ”² Pending | Database layer |
| 2 | ðŸ”² Pending | Web scraper |
| 3 | ðŸ”² Pending | RAG ingestion |
| 4 | ðŸ”² Pending | LangGraph agent |
| 5 | ðŸ”² Pending | FastAPI HTTP layer |
